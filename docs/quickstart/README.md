# 빠른 시작

## 1. 대상 사용자

Ragflow-Plus는 다음과 같은 애플리케이션 시나리오에 적합합니다:

- 대학/기업 등 기관 팀에서 일련의 중앙 지식 베이스를 구축하고 온라인 API를 통해 기존 대규모 모델의 답변 효과를 향상시켜야 하는 경우.

- 로컬에 대규모 모델을 배포하고, 로컬 네트워크를 구축하여 내부적으로 연관된 액세스를 수행하는 경우.

실제 구현 요구 사항을 고려하여 이 프로젝트에서는 Agent 관련 모듈을 제거하고 파일 관리, 파일 파싱 등의 작업을 백엔드로 이동했습니다.

## 2. 하드웨어 요구 사항

배포 하드웨어 리소스는 다음 구성을 충족하는 것이 좋습니다:

- 메모리 요구 사항: 최소 16G 이상, 32G 이상 권장;
- GPU 요구 사항: Turing 아키텍처 이상, 6G VRAM 이상
- 디스크 공간 요구 사항: 20G 이상, SSD 사용 권장

## 3. 배포 시작

Docker를 사용하여 배포하는 것을 권장합니다.

### 1. 프로젝트 코드 복제

```bash
git clone https://github.com/zstar1003/ragflow-plus.git
```

### 2. 이미지 가져오기 및 시작

프로젝트 루트 디렉토리로 이동하여 실행합니다:

```bash
docker compose -f docker/docker-compose.yml up -d
```

이미지 가져오기에 실패하면 다음 Docker 이미지 소스를 구성하여 이미지 가져오기 속도를 높이는 것이 좋습니다.

```bash
"registry-mirrors": [
    "https://docker.m.daocloud.io",
    "https://docker.imgdb.de",
    "https://docker-0.unsee.tech",
    "https://docker.hlmirror.com",
    "https://docker.1ms.run",
    "https://func.ink",
    "https://lispy.org",
    "https://docker.xiaogenban1993.com"
]
```

클라우드 드라이브에 패키징된 이미지 리소스 파일을 직접 다운로드할 수도 있습니다:

클라우드 드라이브 주소(v0.4.3 버전): [https://pan.baidu.com/s/1fC7dzuD0WO3FaEARjJNoMg?pwd=8888](https://pan.baidu.com/s/1fC7dzuD0WO3FaEARjJNoMg?pwd=8888) 추출 코드: 8888 

다음 명령을 사용하여 이미지를 로드하고 설치합니다:
```bash
docker load -i ragflowplus-images.tar
```

### 3. 새 사용자 등록

브라우저에 `http://localhost:8888`를 입력하여 백엔드 관리 시스템에 로그인합니다.

초기 로그인 계정은 `admin`, 비밀번호는 `12345678`입니다.

사용자 관리 메뉴에서 새 사용자를 만듭니다.

이후 임베딩 모델 구성, 지식 베이스 생성 등의 작업이 모두 초기 사용자의 신분으로 실행되므로, 이 사용자를 관리자 사용자로 정의하고, 구체적인 사용자 이름 대신 (admin/lab)과 같은 공용 계정 명명 방식을 사용하는 것이 좋습니다.

![사용자 관리 메뉴 스크린샷](_images/image_1.png)

### 4. 초기 사용자 모델 구성

초기 사용자를 생성한 후, 다음 사용자를 생성하기 전에 먼저 프론트엔드에서 초기 사용자의 모델 설정을 구성하는 것이 좋습니다.

브라우저에 `http://localhost:80`를 입력하여 프론트엔드 사용자 시스템에 로그인합니다.

![모델 구성 스크린샷](_images/image_2.png)

임베딩 모델과 채팅 모델을 구성해야 합니다.

임베딩 모델은 현재 *bge-m3*만 지원하며, ollama 플랫폼을 예로 들면 다음과 같습니다:

먼저 ollama를 통해 bge-m3 모델을 가져옵니다:

```bash
ollama pull bge-m3:latest
```

프론트엔드 구성 시, 모델 이름을 `bge-m3`로 설정하고, url 주소를 `http://host.docker.internal:11434`로 설정합니다.

API를 통해 온라인으로 구성하려면 실리콘 플로우 플랫폼을 사용하는 것이 좋으며, bge-m3 모델은 무료로 호출할 수 있습니다.

플랫폼 주소: https://cloud.siliconflow.cn/i/bjDoFhPf

플랫폼에서 API KEY를 얻어 모델 선택 메뉴에 입력해야 합니다.

채팅 모델 구성도 마찬가지입니다.

### 5. 다른 사용자 생성

초기 사용자를 구성한 후, 백엔드로 돌아가 다른 사용자를 계속 생성할 수 있습니다.

다른 새 사용자는 초기 사용자와 동일한 모델 구성을 사용하며, 자동으로 새 사용자의 팀에 추가됩니다. 팀에 추가된 후에는 지식 베이스 정보를 공유할 수 있습니다.

팀 관리 메뉴를 통해 사용자의 팀 소속을 추가하거나 제거할 수 있습니다.


### 6. 파일 업로드

백엔드 파일 관리 메뉴에서 파일이나 폴더를 업로드할 수 있습니다.

![파일 관리](_images/image_3.png)

### 7. 지식 베이스 구축 및 파싱

백엔드 지식 베이스 관리 메뉴에서 지식 베이스를 생성하고 문서를 추가 및 파싱할 수 있습니다.

파싱하기 전에 먼저 임베딩 모델 구성 테스트를 수행하는 것이 좋습니다.

![임베딩 모델 구성 테스트](_images/image_4.png)

이 인터페이스는 초기 사용자의 최신 임베딩 모델 구성 정보(참고: 시스템 모델 구성의 모델 정보가 아닌 마지막으로 추가된 모델 구성)를 자동으로 읽어옵니다.

연결 테스트가 통과되면 다음 단계인 파일 파싱을 진행할 수 있으며, 파일 개별 파싱과 일괄 파일 파싱 두 가지 방식을 지원합니다.

![파일 파싱 스크린샷](_images/image_5.png)

파싱 로그 정보는 `docker\ragflow-plus-logs\parser.log`에서 출력됩니다.

지원되는 파일 유형은 pdf, doc, docx, pptx, xlsx, csv, txt, md, jpg, png입니다.

### 8. 파싱 내용 확인

파싱이 완료되면 프론트엔드에서 구체적인 파싱 청크 정보를 확인하고 관련 이미지를 조회할 수 있습니다.

![파싱 내용 확인](_images/image_6.png)

### 9. 질의응답 모드

프론트엔드 질의응답 모듈에서 어시스턴트를 생성한 후, 질의응답 상호 작용을 수행하고 텍스트 정보와 텍스트 청크와 관련된 이미지 정보를 출력할 수 있습니다.

![질의응답 모드 출력](_images/image_7.png)

### 10. 문서 작성 모드

프론트엔드 문서 작성 모듈에서 지정된 지식 베이스를 선택하고, 지정된 템플릿에 따라 편집 영역에서 삽입 위치를 선택한 후, 아래 질의응답 영역에 질문을 입력하고 단축키 [Enter]를 사용하여 사용자가 구성한 채팅 모델을 호출하여 문서 작성을 출력하고, 결과를 Word 문서로 내보낼 수 있습니다.

![문서 작성 모드](_images/image_8.png)
